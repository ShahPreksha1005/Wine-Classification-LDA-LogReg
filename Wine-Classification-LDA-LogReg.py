# -*- coding: utf-8 -*-
"""7-8. LDA-LR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H7SIN-ehIrbiASeRvwtOkUXwpW_PzFw-

# **Wine-Classification-LDA-LogReg**

###**Created by Preksha Shah**
"""

#importing the libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""**Wine Dataset:**

* These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the **three types of wines**.
"""

#importing the datasets
df=pd.read_csv("/content/Wine.csv")
df.head()

#Checking for null values
df.isnull().sum()

df.describe()

df.columns

df.info()

df.hist(figsize=(25,20))



plt.figure(figsize=(12,10))
p=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')

from pandas.plotting import scatter_matrix
p=scatter_matrix(df,figsize=(50,40))

plt.figure(figsize=(15,10))
sns.boxplot(df)

x=df.iloc[:,:-1]
x

y=df.iloc[:,-1]
y

## split data into train and test data set

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

from  sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda= LDA(n_components= 2)
x_train = lda.fit_transform(x_train,y_train)
x_test= lda.transform(x_test)

x_train

from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(random_state=0)
classifier.fit(x_train,y_train)
y_pred=classifier.predict(x_test)
y_pred

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
print(cm)

from sklearn.metrics import accuracy_score
acc=accuracy_score(y_test,y_pred)
print("Accuracy=", acc*100, '%')

from matplotlib.colors import ListedColormap
X_Set, Y_Set = x_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_Set[:,0].min() -1, stop = X_Set[:, 0].max() +1, step = 0.01),
                     np.arange(start = X_Set[:,1].min() -1, stop = X_Set[:, 1].max() +1, step = 0.01))

plt.contourf(X1,X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('blue', 'red')))

plt.xlim(X1.min(), X2.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(Y_Set)):
    plt.scatter(X_Set[Y_Set == j, 0], X_Set[Y_Set == j,1],
                c = ListedColormap(('yellow', 'green'))(i), label = j)
plt.title('Logistic Regression ( Training set)')
plt.legend()
plt.show()

"""##**Inferences:**

### 1. **Dataset Overview and Initial Insights**
   - The Wine dataset comprises chemical analysis results for wines from three cultivars grown in the same region of Italy. It includes measurements of 13 constituents, which were used as features for classification.
   - There were no null values in the dataset, indicating it was complete and did not require handling missing data.
   - Descriptive statistics, including histograms, helped visualize the distribution of each feature, and revealed slight variations in distributions among the constituents, which could aid classification.

### 2. **Data Visualization and Correlations**
   - The heatmap of correlations showed varying degrees of correlation among the 13 features. Some features were highly correlated, suggesting multicollinearity. For example, certain chemical properties may have a strong positive or negative relationship with others, likely due to their chemical composition or interaction.
   - The boxplot indicated that some features had outliers or varied distribution ranges, which might impact model performance.

### 3. **Data Preprocessing**
   - The dataset was split into training and test sets, with an 80-20 ratio.
   - Standard scaling was applied to ensure that features had a consistent scale, which is crucial for distance-based algorithms and models sensitive to feature magnitude.
   - Linear Discriminant Analysis (LDA) was used for dimensionality reduction, reducing the feature space from 13 dimensions to 2, helping improve visualization and potentially enhancing classification performance.

### 4. **Model Training and Evaluation**
   - A Logistic Regression model was trained using the transformed features from LDA.
   - The model achieved a high accuracy of approximately **97%** on the test set, suggesting that the features extracted through LDA were effective in distinguishing between wine types.
   - The confusion matrix indicated that most predictions were correct, with only a few misclassifications, showcasing the model’s robustness.

### 5. **Decision Boundary Visualization**
   - The decision boundary visualization showed clear separations among the classes in the 2D space, indicating that the model effectively distinguished between wine types after dimensionality reduction.

---

### **Conclusions**
   - The high accuracy score demonstrates that the chemical constituents of wines can be used effectively to classify them into their respective cultivars.
   - LDA’s dimensionality reduction preserved significant information while improving model interpretability and visualization.
   - Logistic Regression, as a linear classifier, was sufficient to achieve accurate classification, showing that the dataset’s features, especially after LDA, provided strong linear separability.
   - Further exploration with different classification models (like K-Nearest Neighbors or Decision Trees) could be conducted, though Logistic Regression already provides excellent performance.

---
"""